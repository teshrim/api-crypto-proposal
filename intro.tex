\section{Project Description}
\label{sec:intro}

For years, the cryptography research community has been emploring security
engineers and developers to ``please stop rolling your own crypto!''  An
expanded version of plea might be: cryptography is hard, so let the crypto experts
handle the crypto; you just worry about implementing what we provide.  There is
good sense in this message.  Cryptography \emph{is} hard.  And the security
guarantees that we prove can depend crucially on parameter choices, padding
schemes, etc., and in ways that may not be obvious to non-experts. Our
research community has developed a collective wisdom that helps guide us to good
solutions (or at least to avoid pitfalls).

To their credit, security engineers and software developers do largely seem to
have understood.  But there is a response that, sadly, the research community
has heard less clearly: ``correctly implementing your theoretical crypto is hard!''
To paraphrase various industry speakers at editions of Real World Crypto (e.g.,
David McGrew, Daniel Kahn-Gilmore, Thai Duong\cpnote{RWC'17 Wycheproof talk}),
and Paul Kocher in his invited lecture at Crypto'16:
%
Cryptographers need to understand that API/library design is hard work and that,
once fielded, APIs are very slow to change.  (Changes to APIs can break the
applications built upon them, leading to unhappy customers, lost of revenue or
reputation, etc. \cpnote{The most important point we can make here is that
having to change applications using the API costs \emph{real} money. The other
points you make are secondary.})  So no matter how pretty your theory is, if it
is hard to implement with existing APIs, it either won't be implemented, or it
is prone to be implemented incorrectly.  

This proposal adopts the position that \emph{a primary focus of crypto research
should be to deliver primitives and protocols that are as easy as possible to
implement, correctly.}  More specifically, that
\begin{enumerate}
  \item the usefulness and impact of crypto theory will be significantly
    improved if it endeavors to respect the APIs that crypto libraries expose;

  \item wherever possible, crypto theory should be mindful of usability, and
    forgiving of misuse (even when properly implemented);

  \item when theory gives formal syntax for a primitive, it should be clearly
    suggestive of an actual API that is aligned with what developers want; and
    %
    \tsnote{not sure that this is the right wording, or even that it belongs in
    this list (seems a bit more down it the weeds than the others)}
    \cpnote{Agreed. We could say that this is one way to achieve item~2.}

  \item ease of correct implementation should be treated as a first-class
    security property, not dismissed as a mere ``programming exercise''.
    %
    \cpnote{I guess this is the ``pie-in-the-sky''? I'll look at the SWE
    literature and see if I can dig up a way to measure \emph{implementation
    complexity}.}\tsnote{Yep.  But have a look at Rogaway's Partially
  Specified Protocols/Authentication without Ellision paper}
\end{enumerate}

\tsnote{finish out this intro}

\subsection{API-centric Cryptography}
The security community has published extensively on the \emph{usability} of
APIs~\cite{ABF+}\todo{we should have more than one citation}. 
Intuitively, if an API is easy to use correctly, developers are more
likely to build secure applications to top of it.  
%
Our approach is complementary, focusing on the relationships between
functionalities that APIs present (to application developers) and the primitives
that crypto theory provides. We take the position that, in practice, APIs are
primarily driven by non-security requirements (e.g. functionality, adoption,
deployment).
%
As such, it is emcumbent upon us to take an \emph{API-centric} view of the
primitives we formalize.

\paragraph{Prior Work.}
In~\cite{SSW}, PI Shrimpton gave a provable-security treatment to the design and
analysis of so-called ``cryptographic APIs'', standardized in the PKCS\#11
document\cite{xxx}.  That standard details an interface by which one may interact
cryptographic tokens (e.g. smart cards, USB devices, enterprise-grade HSMs) that
are trusted to perform key-manangement duties, enforcement of policies for the
use of stored keys, and a limited number of asymmetric- and symmetric-key crypto
operations.  These tokens support a variety of applications, including
certification authorities, SSL/TLS acceleration, and interbank communication.

Our approach in this paper was API-centric, in that we took the PKCS\#11
standard as the reference point, and defined from it a new cryptographic-API
primitive.  We then formalized some of the intuitive notions of security for a
cryptographic API.  This was a complex undertaking, as one of the core
key-management functionalities is to allow the exporting and importing of keys
via ``key (un)wrapping'' operations.  (This supports, among other things, the
transportation of cryptographically protected keys between tokens.)  These keys
may have a variety of attributes associated to them that proscribe
their usage, e.g. this key may/may not be used to encrypt data (via a given API
call), this key may/may not be wrapped for future export.  Thus, the internal
state of the key-management functionality changes as an attacker carries out
permitted API calls. \todo{Mention the API-attack papers by Graham
  Steele.} 
Moreover, a real adversary may be in possession of one or
more tokens ---~keep in mind that these may be physically vulnerable
devices, such as USB sticks or smart cards~--- and these are subject
to corruption.  As a result, a victim token is asked to
import adverasarially-known keys.  This can cause many other keys
under management by the victim to become (effectively) corrupted, 
e.g. if the attributes of the imported key allow it to be used for wrapping.


%
Recent work by PI Shrimpton~\cite{BPS} revisits the theory of
hedged public-key encryption (PKE)~\cite{BBN+} from an API-centric perspective.
%
The motivation for hedged PKE is quite practical. Traditional
PKE schemes are proved secure under the assumption that they have access to a
source of uniform random bits; but in practice, PKE schemes are implemented on
systems that have faulty RNGs, or where entropy is difficult to harvest.  In
these cases, the security guarantees proved in theory are, at best, voided; at
worst, failure of the RNG can lead to recovery of the plaintext.
%
Hedged PKE is designed to satisfy traditional notions of security when provided
uniform randomness, and still deliver useful security properties as the quality
of randomness degrades.  The most elegant construction of hedged PKE works like
this: synthesize fresh random bits by hashing all of the encryption inputs (the
public key, the message, the provided randomness), and then use these bits as
the randomness for an underlying PKE scheme.  In practice, implementing this
simple construction is surprisingly difficult, as the high- and mid-level APIs
presented by the most commonly used crypto libraries (e.g. OpenSSL and
significant forks thereof) \emph{do not} permit one to specify the
per-encryption randomness.   In~\cite{BPS}, the theory of hedged PKE is
reconsidered from the perspective of what can be easily constructed given what
real APIs expose, and what provable security guarantees these can achieve.

\paragraph{Tasks.} \tsnote{List of possible tasks}
\begin{itemize}
\item As a first step towards respecting real APIs, we must understand the extent to
which the primitives we formalize in theory align with the actual primitives
that APIs expose to developers. {Survey, SoK-type work.}
\item {Application developer APIs for secure send/receive of messges
(data-in-flight) and secure read/write from storage (data-at-rest).  What are
the implied primitives, and what are the security properties that developers
assume are provided.} 
\item {Secure-channel APIs for streaming data vs.
message-oriented data.}  \todo{Look at ``data is a stream'' paper.} 
\item  {Authenticated key-exchange: examine existing protocols and APIs, distill
out design patterns/common features, abstract out the API that one wants as a
security engineer. } 
\item {Close open questions from CrAPI paper}
\end{itemize}

%\paragraph{Prior Work.}

\subsection{Misuse-Forgiving Cryptography}

\paragraph{Prior Work.}
Towards the goal of building cryptography that is forgiving of misuse, prior
work by PI Shrimpton defined the notion of nonce-misuse-resistant authenticated
encryption~\cite{RS06}, and showed how to achieve it using standard
symmetric-key tools.  Rogaway had previously argued~\cite{xxx} that the
classical viewpoint that symmetric encryption schemes are randomized primitives
was misaligned with practice, and what we should be delivering are encryption
schemes that are deterministic, surfacing an explicit IV input. (This is an
early effort to close the gap between a theoretical primitive and its real-world
presentation.)  Moreover, to make schemes easier to use correctly, we should
target security when the IV is a non-repeating value ---~a nonce~--- rather than
demanding the IV be random.  In~\cite{RS06}, we sought to make these nonce-based
encryption schemes even easier to use, by designing them so that their security
guarantee degrades gracefully when the nonce IV repeats.  Nonce-misuse
resistance has been recognized as an important goal,  

Later work by PI Shrimpton~\cite{NRS} reconsidered the traditional wisdom about
building authenticated encryption (AE) via generic composition of an encryption
scheme and a MAC.   The seminal work by Bellare and Namprempre~\cite{BN} showed
that of the three classical compositions ---~encrypt-and-mac, mac-then-encrypt,
encrypt-then-mac~--- only the last is secure given any secure encryption scheme
and MAC.   This wisdom was heeded by an ISO standard, which would have been a
good thing, except that the ISO standard was mandating a nonce-based AE scheme,
whereas the Bellare-Namprempre results were about randomized AE.  As a result,
the ISO scheme was actually broken, despite the standard's (appaudible) efforts
to do what it seemed the crypto community told them.  Here again, the mismatch
between theoretical primitives and their real-world realization was problematic.
In~\cite{NRS}, we readdressed generic composition from the nonce-based
perspective, finding an interesting (albeit less simple) picture of what
compositions are (and are not) generically secure.  (Curiously, one of the
secure compositions that our work uncovered was the SIV-mode previously
published in~\cite{RS06} as the first nonce-misuse-resistant AE scheme.)

\tsnote{Hey Chris: what other types of misuse might be considered?
  Think about not just how one might misuse the inputs (say) to a
  primitive, but also how a primitive might be ``misused'' within an
  application, e.g. there may parallel communication channels, there
  may be metadata in the clear, there may be downgrading or other
  things that happen ``under the hood'' from the application's perspective, etc.}

\paragraph{Tasks.}
\begin{itemize}
\item {Follow-on work to hedging paper}\todo{Chris}
\item {Malleabilty models ideas?}
\item {Survey/user study to find out what things people don't
    understand, want to abuse, find annoying when using libraries/when
  trying to translate theory syntax into APIs}\todo{?}
\end{itemize}

\subsection{Syntax as API}
\tsnote{Most of this came from an earlier approach to writing the
  intro/motivation.  Very drafty.  (As is everything, really, at this point.)}
The modern ``provable security'' approach to cryptrography loosely follows a
three-step recipe: define a precise syntax for the abstract primitive under
study, define formal notions of security for that primitive, realize the
primitive and prove it meets the security notions.  The first step, defining the
syntax, essentially defines the component objects that collectively make up the
primitive, e.g. ``An encryption scheme $\Pi=(\calK,\calE,\calD)$ is a triple of
algorithms...''  The syntax typically describes the number and type of the
inputs and outputs of these components, too, e.g. ``The decryption algorithm
provides a mapping $\calD\colon\bits^{k}\times\bits^* \to \bits^* \cup
\{\bot\}$...'', as well as requirements on the behavior of these components in
any correct realization of them.

In a sense, the syntax for a new primitive provides an API for researchers: for
those wishing to \emph{realize} the primitive (in theory), it makes clear what
are the functionalites that must be realized, and what are the assertions that
must hold for any realization; for those wishing to \emph{use} the primitive, it
makes clear what ``calls'' will be available to them, and what are the expected
inputs and outputs of those calls.
%
Like a real software API, syntax is considered good if it serves the needs of
its consumers, i.e. if it supports clear security notions, facilitates the
writing (and verification) of proofs, and is easily built upon.
%
Like a real software API, syntax for a given cryptographic primitive tends to
have a long life-time (albeit for very different reasons).
%
\ignore{
  Follow-on work typically adopts it, and concerns itself with ``flashier''
  tasks, such as proving tighter security bounds, finding more efficient
  realizations, or moving from an idealized model (e.g. the random-oracle model)
  to the standard model.
}
Indeed, the idea of syntax-as-API is very useful for building intuition, and as
a pedagogical tool.

Of course, this analogy only goes so far.  When a researcher argues for a change
to the accepted syntax ---~a new API for a primitive~--- there is no real cost
incurred if it is adopted.  Prior works using the old syntax do not
suddenly``break''; in the worst case they may (eventually) be viewed as
outdated.  If anything, the new syntax presents an opportunity for researchers
revisit prior work and publish updated findings.

\todo{Can we give an example here?  Say, of how the syntax for
  encryption does not align with real APIs, and what syntax-as-API
  might look like?  Note: I'm not even sure that one needs to go so
  far as to make the syntax look like an API; it might suffice to give
(say, in an appendix) guidance to implementers on how the syntax might
be translated into an API.
}

\tsnote{Random ideas: translating syntax into something more like an API.
  What underlying methods are needed for AEAD, or
  some other common primitive?  For example, decryption:
  RetrieveContext (i.e. some AD may be sent, some might be locally
  stored), ParseCtxt, IsValid,... Also, what about explicitly separating quantities that
  are for ``internal'' use only from those that are ``releasable''?
  This could be especially useful for very high-level primitives/APIs}

\paragraph{Prior Work.} 

\todo{Look at Rogaway's online-AE paper.  I seem to remember him
  saying that the main contribution of that paper was API oriented}

\paragraph{Tasks.}
\begin{itemize}
\item ...
\end{itemize}

\subsection{Ease of Correct Implemenation as Security Property}

\paragraph{Prior Work.}

\tsnote{This thread has
the biggest risk of failure.  But also has potential for large return, as it
would change the way we think about security in the community}
\tsnote{Need to do some digging outside of crypto here!}

\todo{Rogaway's Authentication without Ellision paper}

\paragraph{Tasks.}
\begin{itemize}
\item ...
\end{itemize}

\subsection{Public artifacts and broader impacts}

\subsection{PI Qualifications}

\subsection{Proposal Organization and Tasks}

\ignore{
Contemporary encryption schemes are almost exclusively {\em
distribution-agnostic}. Their security properties are independent of the
statistical characteristics of plaintexts, and they yield ciphertexts that are
uniformly distributed bit strings, irrespective of the use case.
Distribution-agnostic cryptography is conceptually simple and its generality
is often convenient in practice. It fails, however, to meet basic security needs in
several important, real-world contexts.  To address these needs, and
those of applications yet uncovered, we will pursue a line of work centered on
what we call \emph{distribution-sensitive cryptography}.
%This will require a new methodology
%for building context-specific cryptographic schemes, the output of which will be
%  improved security supported by a blend of empirical and formal analyses.



%Password-based encryption
%using a conventional, distribution-agnostic cipher, for example, is vulnerable
%to brute-force cracking---especially given the weak passwords typically chosen
%by users. Similarly, ciphertexts produced using distribution-agnostic encryption
%are distinguishable from typical plaintext messages and thus fail to conceal the
%existence of encrypted communications, i.e., support steganography.

%A need thus exists to advance beyond conventional approaches and devise
%innovative and rigorous cryptographic primitives that are specifically tailored
%to non-uniform plaintext and/or ciphertext distributions. In our proposed work,
%we will both enlarge the existing universe of such primitives and break through
%the conceptual barriers that treat them as isolated notions. We will draw such
%primitives together within a unifying framework that can inform and expand their
%range of application, a framework that we call {\em distribution-sensitive
%cryptography} (DSC).

\paragraph{Motivating problems.}
%Much of our work will be driven
%by three settings in which conventional cryptographic thinking
%has missed the mark, because the tools it has delivered are
%fundamentally mismatched problem presented, or because practical usage
%and deployment issues render them unusable.
Our research agenda will target important problems
for which conventional cryptography has failed to yield adequate solutions.
%tackle several important practical problems for which security
%has been historically elusive.
At first glance, these problems seem unrelated to one another.
%On the contrary, they are connected in a way that calls out for the development of DSC.
Our research in DSC, however, will surface deep underlying connections and overlapping practical challenges in problems such as:

%simultaneously, there exists opportunities for
%for DSC to lead us to
%improvements.  %Examples include censorship circumvention (steganography), securing
%password-based encryption (PBE) against brute-force cracking, and managing
%human-generated (noisy) secrets in authentication. Partial solutions have seen
%deployment, but tend to overlook research-driven results or incorporate them in
%an ad hoc way without rigorous security metrics.
%Conversely, many theoretical
%solutions remain uninformed by empirical study. We will bridge the gap between
%theory and practice in key problem areas by designing novel DSC schemes with
%{\em empirically-driven, provable security}, as for these motivating problems:

\begin{newitemize}

\item{\em Brute-force attacks on password-based encryption.}
  Users tend to select weak passwords. Their password-based encryption (PBE)
  ciphertexts are thus vulnerable to brute-force password cracking attacks
  that try to decrypt under guessed passwords and then check if the resulting
  plaintext is plausible.  This problem is serious and pervasive: In the face
  of today's frequent compromise of mobile devices and cloud systems, PBE is
  often the last line of defense for highly sensitive data.  (Password
    manager~\cite{whitney11} compromise in the cloud is one example arising in
    practice.)
%such as
%password vaults. 
%(At least one password
%management service, LastPass, has already suffered a breach in which PBE vaults
%were apparently exposed~\cite{}\fixme{fill in}.) 
%
%Traditional encryption fails in this setting because it does nothing
%to frustrate efforts to check the plausibility of plaintexts.

%PIs Juels and Ristenpart recently
%proposed {\em honey encryption} (HE)~\cite{HoneyEnc-EC:2014}, a DSC-style
%approach that transcends the brute-force barrier in PBE security. At core it
%requires building schemes that incorporate models of a particular
%application's plaintext distributions. But known HE schemes handle only relatively
%simple message distributions, not the messy ones found in many settings.

%HE strengthens PBE ciphertexts by generating fake plaintexts
%that statistically resemble real ones when decryption occurs under the wrong
%key. Existing HE schemes are narrow (e.g., perform credit-card number
%encryption) or impractical, e.g., yielding ciphertext expansion for RSA private
%keys that is linear in their bit length~\cite{HoneyEnc-EC:2014}. We will
%innovate approaches to HE that radically improve its efficiency and bring it
%into the realm of practicality. For example, we propose below a new approach (a
%``programmable'' PRG construction) that reduces the ciphertext expansion in HE
%for RSA private keys from linear to {\em constant}. 


\item{\em Censorship of encrypted protocols:} Censorship is so pervasive and
heavy-handed in some nations that Reporters Without Borders labels them
``Internet Black Holes''~\cite{NorthKorea:2006}. Deep-packet inspection (DPI)
helps censors identify
and block encrypted network protocols 
%Several nation states conduct focused or blanket censorship by
%using deep-packet inspection (DPI) to detect and block particular protocols,
such as
Tor~\cite{Tor:iran_block-2011,Tor:iran_block,Tor:china_block_one,Tor:china_block,
Tor:china_active_probe,Winter2012,Clayton06ignoringthe},
TLS~\cite{TLS:iran_block}, and Skype VoIP~\cite{China_skype_ban,UAE_skype_ban}.
Anti-censorship tools require encryption primitives capable
of producing ciphertexts that appear to be distributed like ``benign'' cover traffic, 
at least to a level of fidelity that deceives real DPI-based censorship tools
monitoring realistic volumes of traffic. Some existing steganographic tools can achieve
provable assurance that ciphertexts match cover traffic distributions~\cite{Hopper:Provable_Stego,Cachin:Info_Theory_Stego,BC04,lysyanskaya2006provably}, but they
are impractical for most settings.


%\item{\em Censorship of internet traffic.}
%A significant, and increasing, number of nation-states use deep-packet
%inspection (DPI) technology to enforce traffic censorship policies.
%DPI allows censors to base policies on information contained within packet
%payloads.  For example, which protocols are being transported, and
%whether or not encryption is being used (thereby hiding rapid analysis
%of URLs and message text).  Recently
%Tor~\cite{Tor:iran_block-2011,Tor:iran_block,Tor:china_block_one,Tor:china_block,
%Tor:china_active_probe,Winter2012,Clayton06ignoringthe},
%TLS~\cite{TLS:iran_block}, and Skype
%VoIP~\cite{China_skype_ban,UAE_skype_ban} have all been the subject of
%DPI-powered blocking.
%To fight back, anti-censorship tools need encryption primitives
%capable of producing ciphertexts that appear to be distributed like
%`benign' traffic, at least to a level of fidelity that fools real DPI
%when observing realistic volumes of traffic.
%Recent research by PIs Ristenpart and Shrimpton has yielded a primitive called
%format-transforming encryption (FTE)~\cite{..}.  FTE's speed and its ability to
%bypass existing regular-expression-based DPI filters have lead to its 
%deployment already with a number of tools, but it {\em cannot} defeat 
%statistically-based traffic filtering by adversaries with knowledge of normal
%message distributions. Existing relevant steganographic techniques that take
%into account such distributions, moreover, present a gap: They are rigorous but
%impractical~\cite{Hopper:Provable_Stego} or practical but lacking rigorous
%security assurances~\cite{Stegotorus}. %, such as the ability to rule out
%detection
%by certain classes of adversaries. 

%We will develop {\em distribution-matching encryption} techniques that permit
%fast encoding of ciphertexts to match the statistical properties of normal
%traffic and lift the provable security benefits of FTE into a much stronger
%adversarial model. Our research will see foundational benefit from connections
%with honey encryption (see below) in our DSC framework; on the empirical side,
%it will be informed by measurement studies and modeling of real-world internet
%traffic and examination of deployed DPI tools. We will formulate both
%complexity-theoretic adversarial models and empirically-driven ones reflecting
%APIs in censorship tools, andwill use resulting bounds to model and validate
%lightweight classes of DME. We will also use our models to study emerging
%censorship-circumvention approaches, such as tunneling through existing
%communication protocols to resist software-specific and active-probing attacks
%by censors.

\item{\em Securing human-generated authentication secrets.} Users make typos
when they key in passwords. Biometrics, such as fingerprints, are noisy.
Conventional crypto, however, is fragile in the face of error-prone data. 
Existing approaches for cryptographic error-correcting codes such as secure
sketches and fuzzy extractors~\cite{DORS08} seek to address this problem, but leak too much
information about low-entropy user secrets to be of practical use.
%Existing primitives such as secure
%sketches~\cite{} and fuzzy extractors~\cite{} are distribution agnostic, and
%leak less information, but still too much for most practical settings. 
%To
%accommodate noisy data, researchers have developed cryptographic primitives such
%as secure sketches, a component of fuzzy extraction~\cite{}. These primitives
%have not yet seen real-world use, in many cases because they incur excessive
%entropy loss in practice. 
%We will overcome this barrier to adoption through a
%fundamentally new DSC-type approach called a {\em distribution-sensitive secure
%sketch} (DSSS) whose innovation is incorporation into the design of the
%primitive of underlying message distributions. Through empirical study of
%password databases leaked in the wild, we will engineer a practical DSSS scheme
%for password-typo detection whose many applications include more useable honey
%  encryption.  
\end{newitemize}
What is common among these settings is that 
typical cryptographic approaches fail to account for, or leverage to
their benefit, the \textit{distributions} of plaintexts, ciphertexts, and
secrets.
Decryption with a PBE scheme under the wrong
password results in messages that are not distributed like real ones; symmetric
encryption does not produce ciphertexts with the distribution of ``benign''
traffic; and secure sketches and fuzzy extractors cannot capitalize on structure within
distributions of the secret data to which they are applied. 
%These issues here not restricted to particular algorithms, but rather the
%limitations are inherent in the classic security goals targeted by designers: no
%scheme meeting these goals can do better.

\paragraph{Preliminary DSC-style approaches.}
Recent work by the PIs in two of the three problem domains highlights the
promise of DSC-style solutions. PIs Juels and Ristenpart recently introduced
honey encryption (HE), a primitive that yields PBE schemes in which decryption with the
wrong password outputs plausibly distributed decoy plaintexts. The result is provable security in settings where plaintext distributions can be accurately modeled, even when passwords are low-entropy.  While our initial  work on HE introduced
schemes for some simple plaintext distributions, such as
credit-card numbers and prime numbers, extending this work to
other plaintext spaces, such as password managers, documents, and so forth, will require design innovations.

In a separate line of work, PIs Ristenpart and Shrimpton introduced a primitive
called format-transforming encryption (FTE)~\cite{Dyer-2013} for which
ciphertexts appear to be
uniform samples from a regular~\cite{Dyer-2013,luchaup2014libfte} 
or context-free~\cite{luchaup2014formatted} language.
Using appropriate languages for network message formats, the resulting
ciphertexts can be viewed as steganography that mimics 
benign traffic only approximately. The resulting 
schemes, however, are faster than prior steganographic approaches and work against
existing real-world DPI systems. Still, FTE only yields {individual} ciphertexts that
mimic benign traffic formatting, and it does not support non-uniformly 
distributed messages. Thus it is potentially
vulnerable to statistical attacks.  
 
\iffalse
Decryption with a PBE scheme under the wrong password results in
messages that are not distributed like real ones, making plausibility
of plaintexts easy to check.  Recent work on Honey Encryption (HE) by PIs Juels and
Ristenpart begins to address this.  For a few specific applications,
they were able to build PBE schemes for which decryption with the wrong
password outputs plausibly distributed decoy plaintexts. 
%We gave
%proof-of-concept schemes for some simple plaintext distributions, such as
%uniform CCNs and prime numbers.  
Our work will significantly expand upon these results; see Section~\ref{sec:he}

Similarly, conventional encryption does not consider the distribution
of `benign' traffic, so its random bitstring ciphertexts stand
out~\cite{KBMP13}.  Folklore might suggest that steganography 
is the right tool for the job.  However, existing
steganographic tools are either provably secure, yet not suited for
practical uses~\cite{Hopper:Provable_Stego}; or they can support
practical uses, but lack rigorously established security
assurances. (Most traditional steganography falls into the latter category.)
Recent work on Format-Transforming Encryption (FTE) by PIs Ristenpart
and Shrimpton shows that lightweight steganographically enabled
encryption can fool real DPI and support real usage.  Still, FTE only makes
{individual} ciphertexts that mimic `benign' traffic formatting.  
It fails to account for \textit{distributional characteristics} of
traffic, leaving it potentially vulnerable to statistical attacks.  
Our work will address this; see Section~\ref{sec:dse}


Finally, secure
sketches and fuzzy extractors ignore the details of the distribution of secret
data. \tnote{The last one sounds a bit vague. Can we firm it up?}
\tsnote{Parallelize with previous two paragraphs}
%Recent work by the PIs has pointed towards DSC-style solutions in two of these
%domains.  PIs Ristenpart and Shrimpton introduced a primitive called
%format-transforming encryption (FTE)~\cite{..} for which ciphertexts are uniform
%samples from a regular or context-free language. Using appropriate languages for
%network message formats, the resulting ciphertexts can be viewed as crudely
%approximating benign traffic. The resulting schemes are faster 
%than prior steganographic approaches. PIs Juels and Ristenpart recently introduced honey
%encryption (HE) that builds PBE schemes for which decryption with the wrong
%password outputs plausibly distributed decoy plaintexts. We gave
%proof-of-concept schemes for some simple plaintext distributions, such as
%uniform CCNs and prime numbers. 

%FTE's speed and its ability to
%bypass existing regular-expression-based DPI filters have lead to its 
%deployment already with a number of tools, but it {\em cannot} defeat 
%statistically-based traffic filtering by adversaries with knowledge of normal
%message distributions.

%cross-cutting limitation is that existing cryptographic
%primitives are distribution agnostic: in-use symmetric encryption disregards
%normal traffic distributions; PBE tools do not consider plaintext distributions
%and so allow brute-force attackers to pick out the proper message; and secure
%sketches and fuzzy extractors must work for arbitrary high-entropy message
%distributions, not specific ones. 
\fi

While offering promising approaches to addressing the shortcomings of
conventional cryptography, our initial work also highlights the scope of the associated
challenges and the need for bold conceptual advances in developing and
deploying DSC.  Instead of pursuing the challenges in isolation, therefore, we
%propose to develop a broad framework around them.
have brought together a team of PIs to develop a broad framework for DSC. 
We will leverage this framework to develop improved security tools in the
contexts of PBE and censorship avoidance, and also use it to identify and solve
additional problems, such as those arising in the management of noisy
%authentication 
secrets.  
%and for which we only have a plan of attack due to the viewpoints derived from
%this DSC framework.

%goal, with real potential to affect both products \textit{and} people.  But the
%problems will also serve as important tools in the development of a
%theory of DSC, which is the main scientific objective of this work.
%\tsnote{stopped here}

\newpage

\paragraph{Unifying framework.}
%In following sections, we will discuss all of this in much more detail.
%From a structural perspective, 
We will develop DSC through a principled methodological 
blend of hands-on empirical study, cryptographic theory, and
system design. We view this framework itself as a research contribution capable
of complementing and supporting the agendas of other research teams.  
Our framework will consist of four parts:
%The end goal will be a new viewpoint for tackling problems
%We will unify disparate threads of research on DSC concepts
%and primitives into a single, overarching, formal framework. The explanatory
%power of this framework will yield new insights, permit a cross-pollination of
%concepts and techniques, and spawn new tools and techniques for a range of
%practical security problems. The DSC framework tailors and applies
%cryptographic primitives to real-world problem domains methodologically
%through the following sequence of steps:
\begin{newenum}

\item{\em Practice-driven modeling:} A key initial step for any new application will be to 
experimentally characterize real-world adversarial threats. In the
anti-censorship setting, for example,  we will study the capabilities of state-of-the-art DPI
systems, like those used by censors.
At the same time, we will gather or generate datasets to train 
distribution models and also provide testing data for evaluation. 
Example data include real-world network traffic for censorship settings, 
and password leaks like RockYou~\cite{RockYou:2009} for PBE. 
%  All this will in turn educate practically relevant adversarial
%models: what types of attacks are the most important 
%adversaries  
%A class of real-world applications is identified and
%corresponding message distributions estimated, yielding an application model. In
%our examples above, messages of interest include passwords, biometrics, and
%cover traffic. An example model of password selection is the RockYou corpus of
%leaked passwords~\cite{}.

\item{\em Robust, distribution-sensitive definitions:} 
We will develop formal security definitions that are
distribution-sensitive. Generally, this will mean revisiting existing notions
and adapting them to the DSC setting. Using the approach of modern 
provable-security cryptography, we will be able to formally characterize interrelationships among the resulting new
definitions, as well as show feasibility and impossibility results. 
%and accompanying
%definitions / metrics are retooled to achieve or amplify distributional
%sensitivity. Steganography, for example, may be conceptualized as encryption
%with sensitivity to ciphertext distributions, honey encryption as encryption
%with sensitivity to plaintext distributions, and DSSS as a bounded-leakage
%error-detecting (or correcting) code with message-distribution sensitivity. (We
%give formal security definitions for all three below.) 
In addition to distribution-sensitive goals, we will also formalize
``fallback'' security notions that provide best-possible security in case estimates are wrong.

\item{\em Practical constructions and implementation:}  
We will construct distribution-sensitive schemes. These will incorporate
models of application-specific distributions, supporting formal
proofs of security relative to the new DSC and fallback definitions. Performance will be a key consideration. We will
aim to construct practical, easy-to-deploy mechanisms.
%Additionally, a key design principle in DSC is {\em hedging}, i.e., building in
%security assurances against application modeling failures. 

\item{\em Experimental and formal analysis:} Finally, we will analyze the
practicality and security of our constructions. We will build
research prototypes of security tools that incorporate DSC techniques, and
experimentally evaluate the utility of these prototypes.  We will also formally analyze security of
our schemes via reduction-based approaches. Typically this will involve some
assumption about the gap between the primitive-designer's estimate of a
relevant distribution and an adversary's estimate. We will explore new,
reductionist approaches to formal bounds on these gaps, as well as
empirically validate assumptions via appropriate application-specific
experiments, e.g., traffic measurements, analysis of biometric databases, etc. 
\end{newenum}

By developing this framework within the context of several concrete problems,
we will not only provide real security improvements in each setting, but also
bring to light cross-cutting definitions and tools. One common tool that
already emerges is a concept called a {\em distribution-transforming encoder}
(DTE). The lynchpin of the honey encryption construction
in~\cite{HoneyEnc-EC:2014}, a DTE is an encoding scheme whose decoder, given a
uniformly random input bit string, yields a distribution close to a target one
$\mdist$ over a set $\mspace$. The DSC framework points the way to a natural
broadening of DTEs to handle transformations of random variables from one
distribution to another, and subsequently to a DTE definition that supports
use within honey encryption, FTE, steganography, and other primitives. 

This DSC framework also opens up a vista of new cryptographic primitives beyond
those identified in previous work. To handle noisy secrets, we propose later a
new DSC primitive called a {\em distribution-sensitive secure sketch} (DSSS).
Our exploration of DSC has also led us to recognize that password-based
steganography, while used widely in practice, has not received a formal, modern
cryptographic treatment. We propose to rectify this gap and, by incorporating use of an
appropriate DTE, achieve provable steganographic security even for low-entropy
passwords.  

%We will seek out other opportunities, and either work on them
%ourselves or simply advertise implied open problems to the broader research
%community.

%Additionally, as we explain below, a special class of
%DTEs offers a novel approach in some settings to: (3) Highly efficient yet
%provably secure steganography. \noteari{We need to discuss this or drop it.}

%Our DSC framework also brings into view another key conceptual connection,
%between %DSC and techniques from simulation-based proofs of security. These
%proofs often %involve creating an environment that contains a ``trap'' value,
%but is statistically %indistinguishable by an adversary from a real
%environment. Similar techniques can be %used in a DSC primitive to embed a user
%secret (rather than a ``trap'' value) in a %distribution that is
%indistinguishable from a target message distribution. Our HE %construction
%(mentioned above) that yields practical RSA private key encryption is a %simple
%instance of this idea. It gives rise to a key class of DTEs based on rejection
%%sampling that immediately leads to more efficient honey encryption and
%promises %efficiency improvements in steganography and other applications.

\paragraph{Public artifacts and broader impacts.} An explicit step in our work
will be implementation of DSC tools. These tools will aid our research, but will also
serve as a springboard for technology transfer and for impact on
security in practice. The PIs have a strong track record of not only releasing
public, open-source research systems, 
but also going the extra mile to help incorporate such implementations into production
systems, such as Tor and Google's uProxy. 
(See \secref{sec:outreach} for more information about our track record in this
regard.)
We will target similar impact for the
proposed work, the ultimate goal being that users of password management systems, 
activists making use of anti-censorship tools, and
others will benefit from the security improvements that our DSC research will provide. 
 

We will by default also make data sets publicly available, the exception being
cases in which we have privacy or confidentiality obligations. See the Data
Management Plan for more details regarding our handling of data.


\paragraph{Team.} Our efforts are cross-cutting, involving data
analysis, experimental work, and cryptographic theory.   Together, our skills, track record, and momentum 
uniquely qualify us to pursue the
(admittedly lofty) goal of crafting a new approach to cryptographic design.
%, and may borrow tools from variety of
%other expertise areas, such as learning theory and natural language
%processing. 
%
%The right team will be required to make progress toward 
%the admittedly lofty goal of crafting a
%new approach to cryptographic design (one that is complementary to existing
%approaches).
%Beyond being involved in the preliminary DSC efforts, we 
%believe the PIs more broadly have the right mix of expertise and
%collaborative spirit to perform. 
%We have brought together a team with the right backgrounds to
%tackle the development of DSC. 
PIs Juels, Ristenpart and Shrimpton have a lengthy track record of
successful collaborations in security, cryptography, and system
development.  Most recently, Juels and Ristenpart have collaborated on
honey encryption, and Ristenpart and Shrimpton on format-transforming
encryption.  We have collective experience
in both theoretical and empirical exploration of modern security
artifacts, and a shared vision in which experimental research supports good
theory, and vice versa.  
%Given that our inherently broad explorations are likely to draw from diverse
%disciplines, we will seek out area experts with whom to collaborate; 
The PIs also have an extensive history of interdisciplinary research and a mature network of colleagues in other areas that may
benefit our work.
%which
%will be useful when extending cryptographic design methods to incorporate 
%techniques from other disciplines (e.g.\ learning theory, natural
%language processing). 
See the Collaboration Plan for more details about PI backgrounds and our logistical plans.

\paragraph{Proposal Organization and Tasks.} In the following sections, we will
discuss in turn the three key DSC applications mentioned above, along with
three respective solution approaches: honey encryption (HE), a new primitive
called Distribution-Sensitive Encryption (DSE) that generalizes and enhances FTE,
and Distribution-Sensitive Secure Sketches (DSSS). We will then give further
details on our unifying framework, followed by a discussion of the broader
impact of our research agenda.  Throughout, we identify the concrete tasks we
will undertake with the visual call-out {\bf Task}.  Our list of tasks serves
as a compact outline of what we will deliver (at a minimum); a task schedule
may be found in Section~\ref{sec:collabplan}.

}
